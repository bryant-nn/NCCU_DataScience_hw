set.seed(1122)
#input
args = commandArgs(trailingOnly=TRUE)
depth <- args[1]
threshold <- args[2]
depth <- 20
threshold <- 0.5
# load data
data <- read.csv("data.csv")
# load data
data <- read.csv("data.csv")
# load data
data <- read.csv("data.csv")
setwd("~/Documents/110_1/data_science/期末報告")
# load data
data <- read.csv("data.csv")
# shuffle data
data <- data[sample(1:nrow(data)), ]
spec = c(train = .8, validate = .2)
g = sample(cut(
seq(nrow(data)),
nrow(data)*cumsum(c(0,spec)),
labels = names(spec)
))
res = split(data, g)
#train pca
in_d <- res[["train"]]
in_d = in_d[,!colnames(in_d) %in% c('Net.Income.Flag','Bankrupt.')]
pca <- prcomp(in_d, center=TRUE, scale=TRUE)
#--- watch pc
std_dev <- pca$sdev
pr_var <- std_dev^2
prop_varex <- pr_var/sum(pr_var)
plot(prop_varex, type = 'lines')
#--- built train data with Bankrupt and top 40 component
train.data <- data.frame(Bankrupt. = res[["train"]]$Bankrupt., pca$x)
train.data <- train.data[,1:41]
model <- rpart(Bankrupt. ~ .,data = train.data, method = "anova", control=rpart.control(maxdepth= depth),)
val.data <- predict(pca, newdata = res[["validate"]])
val.data <- as.data.frame(val.data)
val.data <- val.data[,1:40]
val <- data.frame(truth = res[["validate"]]$Bankrupt.,
prediction = predict(model, val.data))
val <- mutate(val, pred = ifelse(prediction > threshold, 1, 0))
# confusion matrix of validation
cm <- table(val[,c(1,3)])
print(cm)
TP <- cm[2,2]
TN <- cm[1,1]
FP <- cm[1,2]
FN <- cm[2,1]
accuracy <- (TP+TN)/(TP+FP+FN+TN)
recall <- TP/(TP+FN)
precision <- TP/(TP+FP)
NegativePrecision <- TN/(TN+FN)
df <- data.frame(accuracy = accuracy,
recall = recall,
precision = precision,
NegativePrecision = NegativePrecision)
df <- rbind(cm,df)
accuracy
recall
precision
NegativePrecision
set.seed(2123)
#input
args = commandArgs(trailingOnly=TRUE)
depth <- args[1]
threshold <- args[2]
depth <- 20
threshold <- 0.5
# load data
data <- read.csv("data.csv")
# shuffle data
data <- data[sample(1:nrow(data)), ]
spec = c(train = .8, validate = .2)
g = sample(cut(
seq(nrow(data)),
nrow(data)*cumsum(c(0,spec)),
labels = names(spec)
))
res = split(data, g)
#train pca
in_d <- res[["train"]]
in_d = in_d[,!colnames(in_d) %in% c('Net.Income.Flag','Bankrupt.')]
pca <- prcomp(in_d, center=TRUE, scale=TRUE)
#--- watch pc
std_dev <- pca$sdev
pr_var <- std_dev^2
prop_varex <- pr_var/sum(pr_var)
plot(prop_varex, type = 'lines')
#--- built train data with Bankrupt and top 40 component
train.data <- data.frame(Bankrupt. = res[["train"]]$Bankrupt., pca$x)
train.data <- train.data[,1:41]
model <- rpart(Bankrupt. ~ .,data = train.data, method = "anova", control=rpart.control(maxdepth= depth),)
val.data <- predict(pca, newdata = res[["validate"]])
val.data <- as.data.frame(val.data)
val.data <- val.data[,1:40]
val <- data.frame(truth = res[["validate"]]$Bankrupt.,
prediction = predict(model, val.data))
val <- mutate(val, pred = ifelse(prediction > threshold, 1, 0))
# confusion matrix of validation
cm <- table(val[,c(1,3)])
print(cm)
TP <- cm[2,2]
TN <- cm[1,1]
FP <- cm[1,2]
FN <- cm[2,1]
accuracy <- (TP+TN)/(TP+FP+FN+TN)
recall <- TP/(TP+FN)
precision <- TP/(TP+FP)
NegativePrecision <- TN/(TN+FN)
df <- data.frame(accuracy = accuracy,
recall = recall,
precision = precision,
NegativePrecision = NegativePrecision)
accuracy
recall
precision
setwd("~/data_science/hw7-bryant-nn")
library("randomForest")
library(dplyr)
#input
args = commandArgs(trailingOnly=TRUE)
in_d <- read.csv('hw7_train.csv')
test_data <- read.csv('hw7_test.csv')
#delete the first column
in_d = in_d[,-1]
test_data = test_data[,-1]
# -------------------
#set.seed(5123512)
input_d <- in_d[,-1]
var <- colnames(input_d)
i <- 1
k <- 10
t <- 8346/k
t <- floor(t)
newdf <- data.frame()
tem <- c(1:8346)
set.seed(2123)
max <- 0
while(i<=k){
a <- sample(tem,8346,replace = F)
test_d <- in_d[a[1:t],]
val_d <- in_d[a[(t+1):(2*t)],]
train_d <- in_d[a[((2*t)+1):8346],]
# model using random forest tree
#model <- randomForest(x = train_d[,var], y = as.factor(train_d$label),
#                       ntree = 90,  importance = T)
model <- randomForest(x = train_d[,var], y = (train_d$label),
ntree = 90,  importance = T)
#print(model:train_d)
resultframe <- data.frame(truth=train_d$label,
pred=predict(model, newdata = train_d))
resultframe <- mutate(resultframe, pred = ifelse(pred > 0, 1, -1))
rtab <- table(resultframe)
train_a <- round((sum(diag(rtab))) / (sum(rtab)), 2)
#print(model:val_d)
resultframe <- data.frame(truth=val_d$label,
pred=predict(model,newdata=val_d, type="class"))
rtab <- table(resultframe)
val_a <- round((sum(diag(rtab))) / (sum(rtab)),2)
#print(model:test_d)
resultframe <- data.frame(truth=test_d$label,
pred=predict(model,newdata=test_d, type="class"))
rtab <- table(resultframe)
test_a <- round((sum(diag(rtab))) / (sum(rtab)),2)
f <- "fold"
e <- paste(f,i,sep='')
df <- data.frame(e,train_a,val_a,test_a)
names(df) <-c("set","training","validation","test")
newdf <- rbind(newdf,df)
#store the best model
if( (val_a + test_a) > max){
max = (val_a + test_a)
final_model <- model
}
i <- i+1
}
#dataframe + last row (calculate average)
lastdf <- data.frame("ave.",round( mean(newdf[,2]) , 2),round(mean(newdf[,3]),2),round(mean(newdf[,4]),2))
names(lastdf) <- c("set","training","validation","test")
lastdf <- rbind(newdf,lastdf)
write.table(lastdf, file = 'scoreRF.csv', row.names = F, quote = F,sep = ",")
#make  predict data frame
pred=predict(final_model,newdata=test_data, type="response")
a <- c(0:2086)
predict = data.frame(id = a,label = pred)
model <- randomForest(x = train_d[,var], y = (train_d$label),
ntree = 90,  importance = T)
#---
#make  predict data frame
pred=predict(model,newdata=test_data, type="response")
a <- c(0:2086)
predict = data.frame(id = a,label = pred)
write.table(predict, file='outputRF.csv', row.names = F, quote = F,sep = ",")
model <- randomForest(x = train_d[,var], y = (train_d$label),
ntree = 90,  importance = T)
#---
#make  predict data frame
pred=predict(model,newdata=test_data, type="response")
a <- c(0:2086)
predict = data.frame(id = a,label = pred)
pridict = mutate(predict, label = ifelse(label > 0, 1,-1))
write.table(predict, file='outputRF.csv', row.names = F, quote = F,sep = ",")
View(predict)
in_d <- read.csv('hw7_train.csv')
test_data <- read.csv('hw7_test.csv')
#delete the first column
in_d = in_d[,-1]
test_data = test_data[,-1]
# -------------------
#set.seed(5123512)
input_d <- in_d[,-1]
var <- colnames(input_d)
i <- 1
k <- 10
t <- 8346/k
t <- floor(t)
newdf <- data.frame()
tem <- c(1:8346)
set.seed(2123)
max <- 0
a <- sample(tem,8346,replace = F)
test_d <- in_d[a[1:t],]
val_d <- in_d[a[(t+1):(2*t)],]
train_d <- in_d[a[((2*t)+1):8346],]
model <- randomForest(x = train_d[,var], y = (train_d$label),
ntree = 90,  importance = T)
#---
#make  predict data frame
pred=predict(model,newdata=test_data, type="response")
a <- c(0:2086)
predict = data.frame(id = a,label = pred)
View(predict)
pridict = mutate(predict, label = ifelse(label > 0, 1,-1))
View(predict)
View(pridict)
write.table(predict, file='outputRF.csv', row.names = F, quote = F,sep = ",")
predict = data.frame(id = a,label = pred)
pridictt = mutate(predict, label = ifelse(label > 0, 1,-1))
View(pridictt)
write.table(predictt, file='outputRF.csv', row.names = F, quote = F,sep = ",")
pridictt = mutate(predict, label = ifelse(label > 0, 1,-1))
write.table(predictt, file='outputRF.csv', row.names = F, quote = F,sep = ",")
View(pridict)
View(pridictt)
predictt = mutate(predict, label = ifelse(label > 0, 1,-1))
write.table(predictt, file='outputRF.csv', row.names = F, quote = F,sep = ",")
View(predict)
setwd("~/data_science/hw7-bryant-nn")
url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'
redwine <- read.csv(url,sep = ';')
url1 <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'
whitewine <- read.csv(url1,sep = ';')
data <- rbind(redwine,whitewine)
View(data)
#训练集、测试集划分
set.seed(17)
index <-  which( (1:nrow(data))%%3 == 0 )
train <- data[-index,]
test <- data[index,]
View(test)
library("xgboost")
library("Matrix")
train_matrix <- sparse.model.matrix(quality ~ .-1, data = train)
View(train_matrix)
test_matrix <- sparse.model.matrix(quality ~ .-1, data = test)
train_matrix
View(train)
View(test_data)
View(test)
train_fin <- list(data=train_matrix,label=train_label)
train_matrix <- sparse.model.matrix(quality ~ .-1, data = train)
test_matrix <- sparse.model.matrix(quality ~ .-1, data = test)
train_label <- as.numeric(train$quality>6)
test_label <-  as.numeric(test$quality>6)
train_fin <- list(data=train_matrix,label=train_label)
View(train_fin)
train_label
View(train_fin)
View(model)
View(train_matrix)
train_matrix <- sparse.model.matrix(quality ~ .-1, data = train)
test_matrix <- sparse.model.matrix(quality ~ .-1, data = test)
View(train_matrix)
View(train)
View(train_matrix)
#install.packages("xgboost",repos = "http://cran.us.r-project.org")
library("xgboost")
library("Matrix")
in_d <- read.csv('hw7_train.csv')
test_data <- read.csv('hw7_test.csv')
#delete the first column
in_d = in_d[,-1]
test_data = test_data[,-1]
input_d <- in_d[,-1]
var <- colnames(input_d)
i <- 1
k <- 10
t <- 8346/k
t <- floor(t)
newdf <- data.frame()
tem <- c(1:8346)
set.seed(2123)
max <- 0
while(i<=k){
a <- sample(tem,8346,replace = F)
test_d <- in_d[a[1:t],]
#val_d <- in_d[a[(t+1):(2*t)],]
train_d <- in_d[a[(t+1):8346],]
## Train Model
train_matrix <- sparse.model.matrix(label ~ .-1, data = train_d)
train_label <- as.numeric(train_d$label>0)
train_fin <- list(data=train_matrix,label=train_label)
dtrain <- xgb.DMatrix(data = train_fin$data, label = train_fin$label)
model <- xgboost(data = dtrain,max_depth=6, eta=0.5,
objective='binary:logistic', nround=25)
importance <- model.importance(train_matrix@Dimnames[[2]], model = model)
head(importance)
#print(model:train_d)
resultframe <- data.frame(truth=train_d$label,
pred=predict(model, newdata = train_d))
resultframe <- mutate(resultframe, pred = ifelse(pred > 0.5, 1, -1))
rtab <- table(resultframe)
train_a <- round((sum(diag(rtab))) / (sum(rtab)), 2)
#print(model:val_d)
#print(model:test_d)
resultframe <- data.frame(truth=test_d$label,
pred=predict(model,newdata=test_d))
rtab <- table(resultframe)
test_a <- round((sum(diag(rtab))) / (sum(rtab)),2)
f <- "fold"
e <- paste(f,i,sep='')
df <- data.frame(e,train_a,test_a)
names(df) <-c("set","training","test")
newdf <- rbind(newdf,df)
#store the best model
if( ( test_a) > max){
max = ( test_a)
final_model <- model
}
i <- i+1
}
a <- sample(tem,8346,replace = F)
test_d <- in_d[a[1:t],]
#val_d <- in_d[a[(t+1):(2*t)],]
train_d <- in_d[a[(t+1):8346],]
train_matrix <- sparse.model.matrix(label ~ .-1, data = train_d)
View(train_matrix)
train_label <- as.numeric(train_d$label>0)
train_fin <- list(data=train_matrix,label=train_label)
dtrain <- xgb.DMatrix(data = train_fin$data, label = train_fin$label)
model <- xgboost(data = dtrain,max_depth=6, eta=0.5,
objective='binary:logistic', nround=25)
importance <- model.importance(train_matrix@Dimnames[[2]], model = model)
#训练集、测试集划分
set.seed(17)
index <-  which( (1:nrow(data))%%3 == 0 )
train <- data[-index,]
test <- data[index,]
library("xgboost")
library("Matrix")
train_matrix <- sparse.model.matrix(quality ~ .-1, data = train)
test_matrix <- sparse.model.matrix(quality ~ .-1, data = test)
train_label <- as.numeric(train$quality>6)
test_label <-  as.numeric(test$quality>6)
train_fin <- list(data=train_matrix,label=train_label)
test_fin <- list(data=test_matrix,label=test_label)
dtrain <- xgb.DMatrix(data = train_fin$data, label = train_fin$label)
dtest <- xgb.DMatrix(data = test_fin$data, label = test_fin$label)
#模型训练
xgb <- xgboost(data = dtrain,max_depth=6, eta=0.5,
objective='binary:logistic', nround=25)
#重要重要性排序
importance <- xgb.importance(train_matrix@Dimnames[[2]], model = xgb)
head(importance)
#install.packages("xgboost",repos = "http://cran.us.r-project.org")
library("xgboost")
#install.packages("xgboost",repos = "http://cran.us.r-project.org")
library("xgboost")
library("Matrix")
#input
args = commandArgs(trailingOnly=TRUE)
in_d <- read.csv('hw7_train.csv')
test_data <- read.csv('hw7_test.csv')
#delete the first column
in_d = in_d[,-1]
test_data = test_data[,-1]
input_d <- in_d[,-1]
var <- colnames(input_d)
i <- 1
k <- 10
t <- 8346/k
t <- floor(t)
newdf <- data.frame()
tem <- c(1:8346)
set.seed(2123)
max <- 0
a <- sample(tem,8346,replace = F)
test_d <- in_d[a[1:t],]
#val_d <- in_d[a[(t+1):(2*t)],]
train_d <- in_d[a[(t+1):8346],]
train_matrix <- sparse.model.matrix(label ~ .-1, data = train_d)
train_label <- as.numeric(train_d$label>0)
train_fin <- list(data=train_matrix,label=train_label)
dtrain <- xgb.DMatrix(data = train_fin$data, label = train_fin$label)
#模型训练
xgb <- xgboost(data = dtrain,max_depth=6, eta=0.5,
objective='binary:logistic', nround=25)
#重要重要性排序
importance <- xgb.importance(train_matrix@Dimnames[[2]], model = xgb)
head(importance)
xgb.ggplot.importance(importance)
library(ggplot2)
xgb.ggplot.importance(importance)
pre_xgb = round(predict(xgb,newdata = test_d))
test_matrix <- sparse.model.matrix(test_d)
test_matrix <- sparse.model.matrix(data = test_d)
str(train_matrix)
#install.packages("xgboost",repos = "http://cran.us.r-project.org")
library("xgboost")
library("Matrix")
#input
args = commandArgs(trailingOnly=TRUE)
in_d <- read.csv('hw7_train.csv')
test_data <- read.csv('hw7_test.csv')
#delete the first column
in_d = in_d[,-1]
test_data = test_data[,-1]
input_d <- in_d[,-1]
var <- colnames(input_d)
i <- 1
k <- 10
t <- 8346/k
t <- floor(t)
newdf <- data.frame()
tem <- c(1:8346)
set.seed(2123)
max <- 0
while(i<=k){
a <- sample(tem,8346,replace = F)
test_d <- in_d[a[1:t],]
#val_d <- in_d[a[(t+1):(2*t)],]
train_d <- in_d[a[(t+1):8346],]
## Train Model
train_matrix <- sparse.model.matrix(label ~ .-1, data = train_d)
test_matrix <- sparse.model.matrix(label ~ .-1, data = test_d)
train_label <- as.numeric(train_d$label>0)
test_label <- as.numeric(test_d$label>0)
train_fin <- list(data=train_matrix,label=train_label)
test_fin <- list(data=test_matrix,label=test_label)
dtrain <- xgb.DMatrix(data = train_fin$data, label = train_fin$label)
dtest <- xgb.DMatrix(data = test_fin$data, label = test_fin$label)
#模型训练
xgb <- xgboost(data = dtrain,max_depth=6, eta=0.5,
objective='binary:logistic', nround=25)
#重要重要性排序
importance <- xgb.importance(train_matrix@Dimnames[[2]], model = xgb)
pre_xgb = round(predict(xgb,newdata = dtest))
#print(model:train_d)
#print(model:val_d)
#print(model:test_d)
resultframe <- data.frame(truth=test_d$label,
pred=pre_xgb)
head(resultframe)
rtab <- table(resultframe)
test_a <- round((sum(diag(rtab))) / (sum(rtab)),2)
f <- "fold"
e <- paste(f,i,sep='')
df <- data.frame(e,train_a,test_a)
names(df) <-c("set","training","test")
newdf <- rbind(newdf,df)
#store the best model
if( ( test_a) > max){
max = ( test_a)
final_model <- model
}
i <- i+1
}
#dataframe + last row (calculate average)
lastdf <- data.frame("ave.",round( mean(newdf[,2]) , 2),round(mean(newdf[,3]),2))
names(lastdf) <- c("set","training","test")
lastdf <- rbind(newdf,lastdf)
a <- sample(tem,8346,replace = F)
test_d <- in_d[a[1:t],]
#val_d <- in_d[a[(t+1):(2*t)],]
train_d <- in_d[a[(t+1):8346],]
train_matrix <- sparse.model.matrix(label ~ .-1, data = train_d)
test_matrix <- sparse.model.matrix(label ~ .-1, data = test_d)
train_label <- as.numeric(train_d$label>0)
test_label <- as.numeric(test_d$label>0)
train_fin <- list(data=train_matrix,label=train_label)
test_fin <- list(data=test_matrix,label=test_label)
dtrain <- xgb.DMatrix(data = train_fin$data, label = train_fin$label)
dtest <- xgb.DMatrix(data = test_fin$data, label = test_fin$label)
#模型训练
xgb <- xgboost(data = dtrain,max_depth=6, eta=0.5,
objective='binary:logistic', nround=25)
#重要重要性排序
importance <- xgb.importance(train_matrix@Dimnames[[2]], model = xgb)
pre_xgb = round(predict(xgb,newdata = dtest))
#print(model:test_d)
resultframe <- data.frame(truth=test_d$label,
pred=pre_xgb)
View(resultframe)
resultframe <- mutate(resultframe, pred = ifelse(pred > 0.5, 1, -1))
View(resultframe)
rtab <- table(resultframe)
rtab
test_a <- round((sum(diag(rtab))) / (sum(rtab)),2)
test_a
